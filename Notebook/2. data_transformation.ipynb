{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import sys\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'c:\\\\Pred_maintainance-Project\\\\Notebook'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(\"../\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'c:\\\\Pred_maintainance-Project'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.exception import CustomException\n",
    "from src.logger import logging\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scikit-learn in c:\\pred_maintainance-project\\venv\\lib\\site-packages (1.3.1)\n",
      "Requirement already satisfied: numpy<2.0,>=1.17.3 in c:\\pred_maintainance-project\\venv\\lib\\site-packages (from scikit-learn) (1.26.0)\n",
      "Requirement already satisfied: scipy>=1.5.0 in c:\\pred_maintainance-project\\venv\\lib\\site-packages (from scikit-learn) (1.11.2)\n",
      "Requirement already satisfied: joblib>=1.1.1 in c:\\pred_maintainance-project\\venv\\lib\\site-packages (from scikit-learn) (1.3.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\pred_maintainance-project\\venv\\lib\\site-packages (from scikit-learn) (3.2.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.compose import ColumnTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import RobustScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class DataTransformationConfig:\n",
    "    preprocessor_obj_file_path = os.path.join(\"artifacts\", \"preprocessor.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_cols = ['Engine_no', 'Cycle_no',\n",
    "       'LPC_outlet_temperature',\n",
    "       'HPC_outlet_temperature', 'LPT_outlet_temperature',\n",
    "       'HPC_outlet_pressure',\n",
    "       'Physical_fan_speed', 'Physical_core_speed', \n",
    "       'HPC_outlet_static_pressure', 'Fuel_flow_ratio', 'Fan_speed',\n",
    "       'Bypass_ratio', 'Bleed_enthalpy',\n",
    "       'High_pressure_cool_air_flow', 'Low_pressure_cool_air_flow']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Function for creating a file into pickle\n",
    "\n",
    "def save_object(file_path, obj):\n",
    "    \n",
    "    try:\n",
    "        dir_path = os.path.dirname(file_path)\n",
    "        \n",
    "        os.makedirs(dir_path, exist_ok = True)\n",
    "        \n",
    "        with open(file_path, \"wb\") as file_obj:\n",
    "            pickle.dump(obj, file_obj)\n",
    "                   \n",
    "    except Exception as e:\n",
    "        raise CustomException(e, sys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataTransformation:\n",
    "    def __init__(self):\n",
    "        self.data_transformation_config = DataTransformationConfig()\n",
    "        \n",
    "    def get_data_trasnformer_object(self):\n",
    "        try:\n",
    "            num_cols = ['Engine_no', 'Cycle_no',\n",
    "            'LPC_outlet_temperature',\n",
    "            'HPC_outlet_temperature', 'LPT_outlet_temperature',\n",
    "            'HPC_outlet_pressure',\n",
    "            'Physical_fan_speed', 'Physical_core_speed', \n",
    "            'HPC_outlet_static_pressure', 'Fuel_flow_ratio', 'Fan_speed',\n",
    "            'Bypass_ratio', 'Bleed_enthalpy',\n",
    "            'High_pressure_cool_air_flow', 'Low_pressure_cool_air_flow']\n",
    "            \n",
    "            num_pipeline = Pipeline(\n",
    "                \n",
    "                steps = [\n",
    "                    (\"scaler\", RobustScaler()),\n",
    "                    (\"PCA\", PCA(n_components=0.8))\n",
    "                    \n",
    "                ]\n",
    "            )\n",
    "            \n",
    "            logging.info(\"Pipeline created\")\n",
    "            \n",
    "            preprocessor = ColumnTransformer(\n",
    "                \n",
    "                [\n",
    "                    (\"num_pipeline\", num_pipeline, num_cols)\n",
    "                ]\n",
    "            )\n",
    "            \n",
    "            return preprocessor\n",
    "        \n",
    "        except Exception as e:\n",
    "            raise CustomException(e, sys)\n",
    "        \n",
    "    def initiate_data_transformation(self, train_path, test_path):\n",
    "        \n",
    "        try:\n",
    "            train_df = pd.read_csv(train_path)\n",
    "            test_df = pd.read_csv(test_path)\n",
    "            \n",
    "            train_df[\"RUL\"][train_df[\"RUL\"] > 103] = 103\n",
    "            test_df[\"RUL\"][test_df[\"RUL\"] > 103] = 103\n",
    "            \n",
    "            logging.info(\"Train & test data readed\")\n",
    "            \n",
    "            target_column_name = \"RUL\"\n",
    "            drop_cols = [\n",
    "                'Setting_1', 'Setting_2', 'Setting_3', 'Fan_inlet_temperature', 'Fan_inlet_pressure', \n",
    "                'Bypass_duct_pressure', 'Engine_pressure_ratio', 'Core_speed', 'Burner_fuel_air_ratio', \n",
    "                'Required_fan_speed', 'Required_fan_conversion_speed']  \n",
    "            \n",
    "            \n",
    "            input_feature_train_df = train_df.drop(columns=[target_column_name] + drop_cols)\n",
    "            target_feature_train_df = train_df[target_column_name]\n",
    "            \n",
    "            input_feature_test_df = test_df.drop(columns=[target_column_name] + drop_cols)\n",
    "            target_feature_test_df = test_df[target_column_name]\n",
    "            \n",
    "            \n",
    "            preprocessor_obj = self.get_data_trasnformer_object()\n",
    "            logging.info(\"Applying preprocessing obj on train & test dataframe\")\n",
    "            \n",
    "        #Transforming into Preprocessor object to data\n",
    "            input_feature_train_arr = preprocessor_obj.fit_transform(input_feature_train_df) #train data\n",
    "            input_feature_test_arr = preprocessor_obj.transform(input_feature_test_df) #test data\n",
    "            logging.info(\"Preprocessing applied to training & test datasets\") \n",
    "            \n",
    "            #Converting into numpy array for train & test data\n",
    "            train_arr = np.c_[input_feature_train_arr, np.array(target_feature_train_df)] #train array \n",
    "            test_arr = np.c_[input_feature_test_arr, np.array(target_feature_test_df)] #test array\n",
    "            logging.info(\"Data transfom into the array\")\n",
    "            \n",
    "            #function from utils to save the preprocessor file\n",
    "            save_object(\n",
    "                file_path=self.data_transformation_config.preprocessor_obj_file_path,\n",
    "                obj = preprocessor_obj\n",
    "            )\n",
    "            logging.info(\"Preprocessor file saved as Pickle file\")\n",
    "            \n",
    "            return (\n",
    "                train_arr,\n",
    "                test_arr,\n",
    "                # self.data_transformation_config.preprocessor_obj_file_path,\n",
    "            )\n",
    "            \n",
    "            \n",
    "        except Exception as e:\n",
    "            raise CustomException(e, sys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.predictive_maintenance.components.data_ingestion import DataIngestion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "--- Logging error ---\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Pred_maintainance-Project\\venv\\lib\\logging\\__init__.py\", line 1079, in emit\n",
      "    msg = self.format(record)\n",
      "  File \"c:\\Pred_maintainance-Project\\venv\\lib\\logging\\__init__.py\", line 923, in format\n",
      "    return fmt.format(record)\n",
      "  File \"c:\\Pred_maintainance-Project\\venv\\lib\\logging\\__init__.py\", line 659, in format\n",
      "    record.message = record.getMessage()\n",
      "  File \"c:\\Pred_maintainance-Project\\venv\\lib\\logging\\__init__.py\", line 363, in getMessage\n",
      "    msg = msg % self.args\n",
      "TypeError: not all arguments converted during string formatting\n",
      "Call stack:\n",
      "  File \"c:\\Pred_maintainance-Project\\venv\\lib\\runpy.py\", line 197, in _run_module_as_main\n",
      "    return _run_code(code, main_globals, None,\n",
      "  File \"c:\\Pred_maintainance-Project\\venv\\lib\\runpy.py\", line 87, in _run_code\n",
      "    exec(code, run_globals)\n",
      "  File \"c:\\Pred_maintainance-Project\\venv\\lib\\site-packages\\ipykernel_launcher.py\", line 17, in <module>\n",
      "    app.launch_new_instance()\n",
      "  File \"c:\\Pred_maintainance-Project\\venv\\lib\\site-packages\\traitlets\\config\\application.py\", line 1046, in launch_instance\n",
      "    app.start()\n",
      "  File \"c:\\Pred_maintainance-Project\\venv\\lib\\site-packages\\ipykernel\\kernelapp.py\", line 736, in start\n",
      "    self.io_loop.start()\n",
      "  File \"c:\\Pred_maintainance-Project\\venv\\lib\\site-packages\\tornado\\platform\\asyncio.py\", line 215, in start\n",
      "    self.asyncio_loop.run_forever()\n",
      "  File \"c:\\Pred_maintainance-Project\\venv\\lib\\asyncio\\base_events.py\", line 596, in run_forever\n",
      "    self._run_once()\n",
      "  File \"c:\\Pred_maintainance-Project\\venv\\lib\\asyncio\\base_events.py\", line 1890, in _run_once\n",
      "    handle._run()\n",
      "  File \"c:\\Pred_maintainance-Project\\venv\\lib\\asyncio\\events.py\", line 80, in _run\n",
      "    self._context.run(self._callback, *self._args)\n",
      "  File \"c:\\Pred_maintainance-Project\\venv\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 516, in dispatch_queue\n",
      "    await self.process_one()\n",
      "  File \"c:\\Pred_maintainance-Project\\venv\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 505, in process_one\n",
      "    await dispatch(*args)\n",
      "  File \"c:\\Pred_maintainance-Project\\venv\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 412, in dispatch_shell\n",
      "    await result\n",
      "  File \"c:\\Pred_maintainance-Project\\venv\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 740, in execute_request\n",
      "    reply_content = await reply_content\n",
      "  File \"c:\\Pred_maintainance-Project\\venv\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 422, in do_execute\n",
      "    res = shell.run_cell(\n",
      "  File \"c:\\Pred_maintainance-Project\\venv\\lib\\site-packages\\ipykernel\\zmqshell.py\", line 546, in run_cell\n",
      "    return super().run_cell(*args, **kwargs)\n",
      "  File \"c:\\Pred_maintainance-Project\\venv\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3024, in run_cell\n",
      "    result = self._run_cell(\n",
      "  File \"c:\\Pred_maintainance-Project\\venv\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3079, in _run_cell\n",
      "    result = runner(coro)\n",
      "  File \"c:\\Pred_maintainance-Project\\venv\\lib\\site-packages\\IPython\\core\\async_helpers.py\", line 129, in _pseudo_sync_runner\n",
      "    coro.send(None)\n",
      "  File \"c:\\Pred_maintainance-Project\\venv\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3284, in run_cell_async\n",
      "    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
      "  File \"c:\\Pred_maintainance-Project\\venv\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3466, in run_ast_nodes\n",
      "    if await self.run_code(code, result, async_=asy):\n",
      "  File \"c:\\Pred_maintainance-Project\\venv\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3526, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"C:\\Users\\ASUS\\AppData\\Local\\Temp\\ipykernel_3140\\3028050073.py\", line 3, in <module>\n",
      "    train_data, test_data = obj.initiate_data_ingestion()\n",
      "  File \"c:\\Pred_maintainance-Project\\src\\predictive_maintenance\\components\\data_ingestion.py\", line 33, in initiate_data_ingestion\n",
      "    df = read_sql_data()\n",
      "  File \"c:\\Pred_maintainance-Project\\src\\predictive_maintenance\\utils.py\", line 30, in read_sql_data\n",
      "    logging.info(\"Connection establish\",mydb)\n",
      "Message: 'Connection establish'\n",
      "Arguments: (<pymysql.connections.Connection object at 0x000002026A59FFA0>,)\n",
      "c:\\Pred_maintainance-Project\\src\\predictive_maintenance\\utils.py:32: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  df_train = pd.read_sql_query(\"select * from train1\", mydb)\n",
      "c:\\Pred_maintainance-Project\\src\\predictive_maintenance\\utils.py:34: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  df_test = pd.read_sql_query(\"select * from test1\", mydb)\n",
      "C:\\Users\\ASUS\\AppData\\Local\\Temp\\ipykernel_3140\\2777601936.py:45: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  train_df[\"RUL\"][train_df[\"RUL\"] > 103] = 103\n",
      "C:\\Users\\ASUS\\AppData\\Local\\Temp\\ipykernel_3140\\2777601936.py:46: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  test_df[\"RUL\"][test_df[\"RUL\"] > 103] = 103\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    obj = DataIngestion()\n",
    "    train_data, test_data = obj.initiate_data_ingestion()\n",
    "    \n",
    "    data_transformation = DataTransformation()\n",
    "    data_transformation.initiate_data_transformation(train_data, test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
